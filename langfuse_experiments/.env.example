# ============================================================
# Langfuse Experiments — Environment Variable Template
# Copy this file to .env and fill in real values.
# NEVER commit your actual .env file to version control.
# ============================================================

# ----- Langfuse (required) ----------------------------------
# Get from: https://cloud.langfuse.com → Project → Settings → API Keys
LANGFUSE_PUBLIC_KEY="pk-lf-..."
LANGFUSE_SECRET_KEY="sk-lf-..."
LANGFUSE_BASE_URL="https://us.cloud.langfuse.com"

# ----- OpenAI-compatible LLM (required) ---------------------
# Used by the framework's AsyncClientManager for:
#   1. Running the agent (OPENAI_BASE_URL points to Gemini)
#   2. LLM-as-judge evaluator calls
OPENAI_BASE_URL="https://generativelanguage.googleapis.com/v1beta/openai/"
OPENAI_API_KEY="..."       # Or use GEMINI_API_KEY or GOOGLE_API_KEY

# # ----- Gemini / Google (required for default agent) ---------
# GEMINI_API_KEY="..."       # Same key accepted via GOOGLE_API_KEY
# GOOGLE_API_KEY="..."       # Accepted as alias for GEMINI_API_KEY

# ----- Model selection (optional, has defaults) -------------
DEFAULT_PLANNER_MODEL="gemini-2.5-pro"
DEFAULT_WORKER_MODEL="gemini-2.5-flash"
DEFAULT_EVALUATOR_MODEL="gemini-2.5-pro"   # Judge model

# ----- Report Generation Database (required for agent) ------
# Path is relative to where you run the script from (repo root)
REPORT_GENERATION_DB__DRIVER="sqlite"
REPORT_GENERATION_DB__DATABASE="implementations/report_generation/data/OnlineRetail.db"
REPORT_GENERATION_DB__QUERY__MODE="ro"

# ----- Experiment settings (optional, overridable via CLI) --
# These can be overridden with --dataset-name and --experiment-name CLI flags
LANGFUSE_DATASET_NAME="OnlineRetailReportEval"
EXPERIMENT_NAME="report-generation-baseline"
REPORT_GENERATION_LANGFUSE_PROJECT_NAME="Report Generation"
REPORT_GENERATION_OUTPUT_PATH="implementations/report_generation/reports/"
