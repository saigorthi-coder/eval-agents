"""SQL-quality trace evaluator.

Evaluates the quality of SQL queries generated by the agent.
Performs a lightweight deterministic syntax check first (via sqlparse),
then calls the LLM judge for semantic quality assessment.

Langfuse score name: ``sql_quality``  (0.0 – 1.0)
Returns ``[]`` if no SQL queries are found in the trace.
"""

import logging
from typing import Any

from aieng.agent_evals.evaluation.graders.config import LLMRequestConfig
from aieng.agent_evals.evaluation.types import Evaluation
from langfuse.api.resources.commons.types.trace_with_full_details import TraceWithFullDetails
from langfuse.experiment import ExperimentItemResult

from langfuse_experiments.config.experiment_config import ExperimentConfig
from langfuse_experiments.evaluators.base_evaluator import (
    JudgeResponse,
    call_llm_judge,
    extract_sql_from_trace,
    get_question,
    score_to_float,
)

logger = logging.getLogger(__name__)

_CFG = ExperimentConfig()


def _syntax_check(sql: str) -> bool:
    """Return True if sqlparse can parse the statement without errors."""
    try:
        import sqlparse

        parsed = sqlparse.parse(sql.strip())
        return bool(parsed) and all(bool(stmt.tokens) for stmt in parsed)
    except Exception:
        return True  # If sqlparse is unavailable, assume valid


_SYSTEM_PROMPT = """\
You are an expert SQL evaluator assessing SQL queries generated by an AI data analytics agent.

The database contains an OnlineRetail dataset with columns such as InvoiceNo, StockCode, Description,
Quantity, InvoiceDate, UnitPrice, CustomerID, Country.

Evaluate step by step:
1. Is the SQL syntactically valid?
2. Are the referenced columns and tables correct for the schema?
3. Is the query aligned with the user's intent?
4. Are there any inefficiencies (e.g., missing aggregations, wrong filters)?

Score SQL quality from 1–5:
5 = Valid, correct, efficient, fully aligned with intent
4 = Valid and mostly correct, minor issues
3 = Mostly valid, minor syntax errors or misaligned logic
2 = Significant errors or wrong tables/columns
1 = Invalid SQL or completely off-topic

Respond with valid JSON only (no markdown fences):
{{"reasoning": "step-by-step analysis", "score": <1-5>}}
"""


def _make_user_prompt(question: str, sql_queries: list[str]) -> str:
    queries_block = "\n\n".join(f"Query {i}:\n{q}" for i, q in enumerate(sql_queries, 1))
    return f"USER QUESTION:\n{question}\n\nGENERATED SQL QUERIES:\n{queries_block}"


async def sql_quality_evaluator(
    *,
    trace: TraceWithFullDetails,
    item_result: ExperimentItemResult,
    **kwargs: Any,
) -> list[Evaluation]:
    """Evaluate the quality of SQL queries in the trace.

    Returns ``[]`` if no SQL queries are detected.

    Parameters
    ----------
    trace : TraceWithFullDetails
        Full trace with observations.
    item_result : ExperimentItemResult
        Experiment item result (for question extraction).

    Returns
    -------
    list[Evaluation]
        ``[Evaluation(name="sql_quality", value=0.0–1.0)]`` or ``[]``.
    """
    question = get_question(trace, item_result)
    sql_observations = extract_sql_from_trace(trace)

    if not sql_observations:
        logger.info("sql_quality_evaluator: no SQL found in trace %s — skipping", trace.id)
        return []

    sql_queries = [obs["sql"] for obs in sql_observations]

    # --- Deterministic pre-check: syntax validation ---
    syntax_failures = [q for q in sql_queries if not _syntax_check(q)]
    if syntax_failures:
        logger.warning("sql_quality_evaluator: %d queries failed syntax check", len(syntax_failures))

    model_config = LLMRequestConfig(
        model=_CFG.judge_model,
        temperature=_CFG.judge_temperature,
        max_completion_tokens=_CFG.judge_max_tokens,
        retry_max_attempts=_CFG.judge_retry_max_attempts,
        retry_initial_wait_sec=_CFG.judge_retry_initial_wait_sec,
        retry_max_wait_sec=_CFG.judge_retry_max_wait_sec,
        retry_backoff_multiplier=_CFG.judge_retry_backoff_multiplier,
    )

    try:
        response: JudgeResponse = await call_llm_judge(
            system_prompt=_SYSTEM_PROMPT,
            user_prompt=_make_user_prompt(question or "(unknown)", sql_queries),
            model_config=model_config,
        )
    except Exception as exc:
        logger.error("sql_quality_evaluator: judge call failed for trace %s: %s", trace.id, exc)
        return []

    score = score_to_float(response.score)
    logger.debug("sql_quality: trace=%s score=%d (%.2f)", trace.id, response.score, score)

    return [
        Evaluation(
            name="sql_quality",
            value=score,
            comment=f"[{response.score}/5] {response.reasoning}",
            metadata={
                "sql_count": len(sql_queries),
                "syntax_failures": len(syntax_failures),
            },
        )
    ]
